{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "import requests\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode can be either 'train' or 'infer'\n",
    "# Set to 'infer' will skip the training\n",
    "MODE = 'train'\n",
    "URL = 'http://www.manythings.org/anki/fra-eng.zip'\n",
    "FILENAME = 'yue-eng.zip'\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 256\n",
    "RNN_SIZE = 512\n",
    "NUM_EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the score function to compute alignment vectors\n",
    "# Can choose between 'dot', 'general' or 'concat'\n",
    "ATTENTION_FUNC = 'concat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_and_read_file(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        session = requests.Session()\n",
    "        response = session.get(url, stream=True)\n",
    "\n",
    "        CHUNK_SIZE = 32768\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    zipf = ZipFile(filename)\n",
    "    filename = zipf.namelist()\n",
    "    with zipf.open('yue.txt') as f:\n",
    "        lines = f.read()\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = maybe_download_and_read_file(URL, FILENAME)\n",
    "lines = lines.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Why do you insist on paying for your school expenses yourself, when your parents are willing to give you financial support?', '你阿爸阿媽都肯喺財政上支持你啦，點解你仲係都要自己俾學費啫？', 'CC-BY 2.0 (France) Attribution: tatoeba.org #325648 (CK) & #5777423 (nickyeow)'], [\"You only notice those who are loud and obnoxious. You don't notice the majority who are just normal people and keep to themselves.\", '你淨係會留意到嗰啲嘈嘈閉、好乞人憎嘅人，唔會留意到絕大部份安安靜靜咁自己做自己嘢嘅正常人。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #6045678 (mailohilohi) & #6488761 (nickyeow)'], ['If you take a child outside and point at the moon, they will look at the moon. If you do the same thing with a dog, it will look at your finger.', '如果你帶個細路出街，指住個月亮，佢會望住個月亮。如果你對隻狗做同樣嘅嘢，佢會望住你隻手指。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #5264594 (Cithara) & #5798410 (nickyeow)'], [\"The people here are particular about what they eat, so even if a restaurant is inexpensive, it'll soon go out of business if the food doesn't taste good.\", '呢到嘅人對食好講究，所以就算係幾平嘅餐廳都好，如果啲嘢食唔好食，都會好快執笠。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2518462 (CK) & #3868875 (nickyeow)'], ['']]\n"
     ]
    }
   ],
   "source": [
    "raw_data = []\n",
    "for line in lines.split('\\n'):\n",
    "    raw_data.append(line.split('\\t'))\n",
    "\n",
    "print(raw_data[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"When I was studying to become a lawyer, my teachers told me to never ask a question that I didn't know the answer to.\", '我讀法律嗰陣，啲老師教我唔好問啲自己唔知道答案嘅問題。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #1989692 (CK) & #5752926 (nickyeow)'], ['Why do you insist on paying for your school expenses yourself, when your parents are willing to give you financial support?', '你阿爸阿媽都肯喺財政上支持你啦，點解你仲係都要自己俾學費啫？', 'CC-BY 2.0 (France) Attribution: tatoeba.org #325648 (CK) & #5777423 (nickyeow)'], [\"You only notice those who are loud and obnoxious. You don't notice the majority who are just normal people and keep to themselves.\", '你淨係會留意到嗰啲嘈嘈閉、好乞人憎嘅人，唔會留意到絕大部份安安靜靜咁自己做自己嘢嘅正常人。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #6045678 (mailohilohi) & #6488761 (nickyeow)'], ['If you take a child outside and point at the moon, they will look at the moon. If you do the same thing with a dog, it will look at your finger.', '如果你帶個細路出街，指住個月亮，佢會望住個月亮。如果你對隻狗做同樣嘅嘢，佢會望住你隻手指。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #5264594 (Cithara) & #5798410 (nickyeow)'], [\"The people here are particular about what they eat, so even if a restaurant is inexpensive, it'll soon go out of business if the food doesn't taste good.\", '呢到嘅人對食好講究，所以就算係幾平嘅餐廳都好，如果啲嘢食唔好食，都會好快執笠。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2518462 (CK) & #3868875 (nickyeow)']]\n"
     ]
    }
   ],
   "source": [
    "# The last element is empty, so omit it\n",
    "raw_data = raw_data[:-1]\n",
    "print(raw_data[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s)\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_en, raw_data_cantonese, info = list(zip(*raw_data))\n",
    "raw_data_en = [normalize_string(data) for data in raw_data_en]\n",
    "raw_data_cantonese_in = ['<start> ' + normalize_string(data) for data in raw_data_cantonese]\n",
    "raw_data_cantonese_out = [normalize_string(data) + ' <end>' for data in raw_data_cantonese]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sequences\n",
      "[[ 960    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [1381  135    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(raw_data_en)\n",
    "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')\n",
    "print('English sequences')\n",
    "print(data_en[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantonese input sequences\n",
      "[[1 0 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cantonese_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "cantonese_tokenizer.fit_on_texts(raw_data_cantonese_in)\n",
    "cantonese_tokenizer.fit_on_texts(raw_data_cantonese_out)\n",
    "data_cantonese_in = cantonese_tokenizer.texts_to_sequences(raw_data_cantonese_in)\n",
    "data_cantonese_in = tf.keras.preprocessing.sequence.pad_sequences(data_cantonese_in, padding='post')\n",
    "print('cantonese input sequences')\n",
    "print(data_cantonese_in[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French output sequences\n",
      "[[2 0 0 0]\n",
      " [2 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "data_cantonese_out = fr_tokenizer.texts_to_sequences(raw_data_cantonese_out)\n",
    "data_cantonese_out = tf.keras.preprocessing.sequence.pad_sequences(data_cantonese_out,\n",
    "                                                            padding='post')\n",
    "print('French output sequences')\n",
    "print(data_cantonese_out[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data_en, data_cantonese_in, data_cantonese_out))\n",
    "dataset = dataset.shuffle(len(raw_data_en)).batch(\n",
    "    BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.rnn_size = rnn_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            rnn_size, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, sequence, states):\n",
    "        embed = self.embedding(sequence)\n",
    "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
    "\n",
    "        return output, state_h, state_c\n",
    "\n",
    "    def init_states(self, batch_size):\n",
    "        return (tf.zeros([batch_size, self.rnn_size]),\n",
    "                tf.zeros([batch_size, self.rnn_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "\n",
    "encoder = Encoder(en_vocab_size, EMBEDDING_SIZE, RNN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, rnn_size, attention_func):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.attention_func = attention_func\n",
    "\n",
    "        if attention_func not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(\n",
    "                'Unknown attention score function! Must be either dot, general or concat.')\n",
    "\n",
    "        if attention_func == 'general':\n",
    "            # General score function\n",
    "            self.wa = tf.keras.layers.Dense(rnn_size)\n",
    "        elif attention_func == 'concat':\n",
    "            # Concat score function\n",
    "            self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
    "            self.va = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_output, encoder_output):\n",
    "        if self.attention_func == 'dot':\n",
    "            # Dot score function: decoder_output (dot) encoder_output\n",
    "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
    "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
    "            # => score has shape: (batch_size, 1, max_len)\n",
    "            score = tf.matmul(decoder_output, encoder_output, transpose_b=True)\n",
    "        elif self.attention_func == 'general':\n",
    "            # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n",
    "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
    "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
    "            # => score has shape: (batch_size, 1, max_len)\n",
    "            score = tf.matmul(decoder_output, self.wa(\n",
    "                encoder_output), transpose_b=True)\n",
    "        elif self.attention_func == 'concat':\n",
    "            # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n",
    "            # Decoder output must be broadcasted to encoder output's shape first\n",
    "            decoder_output = tf.tile(\n",
    "                decoder_output, [1, encoder_output.shape[1], 1])\n",
    "\n",
    "            # Concat => Wa => va\n",
    "            # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n",
    "            score = self.va(\n",
    "                self.wa(tf.concat((decoder_output, encoder_output), axis=-1)))\n",
    "\n",
    "            # Transpose score vector to have the same shape as other two above\n",
    "            # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n",
    "            score = tf.transpose(score, [0, 2, 1])\n",
    "\n",
    "        # alignment a_t = softmax(score)\n",
    "        alignment = tf.nn.softmax(score, axis=2)\n",
    "\n",
    "        # context vector c_t is the weighted average sum of encoder output\n",
    "        context = tf.matmul(alignment, encoder_output)\n",
    "\n",
    "        return context, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_size, attention_func):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attention = LuongAttention(rnn_size, attention_func)\n",
    "        self.rnn_size = rnn_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            rnn_size, return_sequences=True, return_state=True)\n",
    "        self.wc = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
    "        self.ws = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, sequence, state, encoder_output):\n",
    "        # Remember that the input to the decoder\n",
    "        # is now a batch of one-word sequences,\n",
    "        # which means that its shape is (batch_size, 1)\n",
    "        embed = self.embedding(sequence)\n",
    "\n",
    "        # Therefore, the lstm_out has shape (batch_size, 1, rnn_size)\n",
    "        lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n",
    "\n",
    "        # Use self.attention to compute the context and alignment vectors\n",
    "        # context vector's shape: (batch_size, 1, rnn_size)\n",
    "        # alignment vector's shape: (batch_size, 1, source_length)\n",
    "        context, alignment = self.attention(lstm_out, encoder_output)\n",
    "\n",
    "        # Combine the context vector and the LSTM output\n",
    "        # Before combined, both have shape of (batch_size, 1, rnn_size),\n",
    "        # so let's squeeze the axis 1 first\n",
    "        # After combined, it will have shape of (batch_size, 2 * rnn_size)\n",
    "        lstm_out = tf.concat(\n",
    "            [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
    "\n",
    "        # lstm_out now has shape (batch_size, rnn_size)\n",
    "        lstm_out = self.wc(lstm_out)\n",
    "\n",
    "        # Finally, it is converted back to vocabulary space: (batch_size, vocab_size)\n",
    "        logits = self.ws(lstm_out)\n",
    "\n",
    "        return logits, state_h, state_c, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantonese_vocab_size = len(cantonese_tokenizer.word_index) + 1\n",
    "\n",
    "decoder = Decoder(cantonese_vocab_size, EMBEDDING_SIZE, RNN_SIZE, ATTENTION_FUNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines can be used for debugging purpose\n",
    "# Or can be seen as a way to build the models\n",
    "initial_state = encoder.init_states(1)\n",
    "encoder_outputs = encoder(tf.constant([[1]]), initial_state)\n",
    "decoder_outputs = decoder(tf.constant(\n",
    "    [[1]]), encoder_outputs[1:], encoder_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(targets, logits):\n",
    "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_source_text=None):\n",
    "    if test_source_text is None:\n",
    "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
    "    print(test_source_text)\n",
    "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
    "    print(test_source_seq)\n",
    "\n",
    "    en_initial_states = encoder.init_states(1)\n",
    "    en_outputs = encoder(tf.constant(test_source_seq), en_initial_states)\n",
    "\n",
    "    de_input = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
    "    de_state_h, de_state_c = en_outputs[1:]\n",
    "    out_words = []\n",
    "    alignments = []\n",
    "\n",
    "    while True:\n",
    "        de_output, de_state_h, de_state_c, alignment = decoder(\n",
    "            de_input, (de_state_h, de_state_c), en_outputs[0])\n",
    "        de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
    "        out_words.append(fr_tokenizer.index_word[de_input.numpy()[0][0]])\n",
    "\n",
    "        alignments.append(alignment.numpy())\n",
    "\n",
    "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
    "            break\n",
    "\n",
    "    print(' '.join(out_words))\n",
    "    return np.array(alignments), test_source_text.split(' '), out_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(source_seq, target_seq_in, target_seq_out, en_initial_states):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        en_outputs = encoder(source_seq, en_initial_states)\n",
    "        en_states = en_outputs[1:]\n",
    "        de_state_h, de_state_c = en_states\n",
    "\n",
    "        # We need to create a loop to iterate through the target sequences\n",
    "        for i in range(target_seq_out.shape[1]):\n",
    "            # Input to the decoder must have shape of (batch_size, length)\n",
    "            # so we need to expand one dimension\n",
    "            decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n",
    "            logit, de_state_h, de_state_c, _ = decoder(\n",
    "                decoder_in, (de_state_h, de_state_c), en_outputs[0])\n",
    "\n",
    "            # The loss is now accumulated through the whole batch\n",
    "            loss += loss_func(target_seq_out[:, i], logit)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss / target_seq_out.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('checkpoints_luong/encoder'):\n",
    "    os.makedirs('checkpoints_luong/encoder')\n",
    "if not os.path.exists('checkpoints_luong/decoder'):\n",
    "    os.makedirs('checkpoints_luong/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines for inference mode\n",
    "encoder_checkpoint = tf.train.latest_checkpoint('checkpoints_luong/encoder')\n",
    "decoder_checkpoint = tf.train.latest_checkpoint('checkpoints_luong/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.9257\n",
      "Are you going to eat all of that yourself ?\n",
      "[[29, 3, 94, 5, 102, 63, 16, 15, 192, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 2 Batch 0 Loss 0.0258\n",
      "You can take a horse to water but you can t make him drink .\n",
      "[[3, 24, 80, 7, 659, 5, 199, 97, 3, 24, 9, 150, 48, 234, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 3 Batch 0 Loss 0.0048\n",
      "Your composition was good except for the spelling .\n",
      "[[25, 2278, 30, 70, 764, 19, 4, 1354, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 4 Batch 0 Loss 0.0014\n",
      "He forgot to lock the door .\n",
      "[[10, 319, 5, 1075, 4, 206, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 5 Batch 0 Loss 0.0008\n",
      "I ve been looking for a job a long time .\n",
      "[[2, 77, 107, 211, 19, 7, 165, 7, 154, 46, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 6 Batch 0 Loss 0.0019\n",
      "We took turns driving the car .\n",
      "[[26, 188, 1137, 691, 4, 119, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 7 Batch 0 Loss 0.0003\n",
      "We need carrots and tomatoes for the soup .\n",
      "[[26, 106, 849, 47, 2051, 19, 4, 685, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 8 Batch 0 Loss 0.0381\n",
      "Your purse is similar to mine .\n",
      "[[25, 1140, 8, 1141, 5, 395, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 9 Batch 0 Loss 0.0020\n",
      "After a six month period his leg was healed and is normal again .\n",
      "[[105, 7, 356, 380, 2474, 37, 2475, 30, 2476, 47, 8, 1375, 225, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 10 Batch 0 Loss 0.0011\n",
      "Why do you know that ?\n",
      "[[71, 22, 3, 45, 15, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 11 Batch 0 Loss 0.0006\n",
      "Tom went back to his hometown .\n",
      "[[27, 168, 181, 5, 37, 1081, 1]]\n",
      "tom <end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 12 Batch 0 Loss 0.0001\n",
      "He got up an hour early this morning .\n",
      "[[10, 138, 52, 72, 293, 245, 18, 153, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 13 Batch 0 Loss 0.0001\n",
      "He is too young to go there alone .\n",
      "[[10, 8, 93, 371, 5, 41, 40, 390, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 14 Batch 0 Loss 0.0002\n",
      "When I want to cry I think of you .\n",
      "[[59, 2, 44, 5, 383, 2, 60, 16, 3, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 15 Batch 0 Loss 0.0003\n",
      "I study abroad .\n",
      "[[2, 235, 477, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 16 Batch 0 Loss 0.0001\n",
      "All of a sudden the door shut with a bang .\n",
      "[[63, 16, 7, 2055, 4, 206, 686, 38, 7, 2056, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 17 Batch 0 Loss 0.0001\n",
      "I don t like spicy food .\n",
      "[[2, 20, 9, 35, 568, 213, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 18 Batch 0 Loss 0.0004\n",
      "No one was hurt .\n",
      "[[64, 81, 30, 537, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 19 Batch 0 Loss 0.0000\n",
      "I had no idea there d be this many people here .\n",
      "[[2, 56, 64, 442, 40, 130, 34, 18, 131, 117, 58, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 20 Batch 0 Loss 0.0009\n",
      "I m going through my closet to find clothes to give to charity .\n",
      "[[2, 36, 94, 1374, 17, 2457, 5, 224, 1343, 5, 108, 5, 2458, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 21 Batch 0 Loss 0.0111\n",
      "I don t like heavy makeup on a young girl .\n",
      "[[2, 20, 9, 35, 558, 2032, 23, 7, 371, 241, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 22 Batch 0 Loss 0.0001\n",
      "Would you please not leave the door open ?\n",
      "[[65, 3, 62, 32, 243, 4, 206, 318, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 23 Batch 0 Loss 0.0003\n",
      "All you have to do is just sit there and listen . You won t have to say anything .\n",
      "[[63, 3, 21, 5, 22, 8, 98, 351, 40, 47, 956, 1, 3, 180, 9, 21, 5, 115, 146, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 24 Batch 0 Loss 0.0013\n",
      "Double the dose .\n",
      "[[638, 4, 975, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 25 Batch 0 Loss 0.0133\n",
      "Tom is as tall as his father .\n",
      "[[27, 8, 66, 788, 66, 37, 99, 1]]\n",
      "tom <end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 26 Batch 0 Loss 0.0002\n",
      "I know Tom quite well .\n",
      "[[2, 45, 27, 796, 169, 1]]\n",
      "tom <end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 27 Batch 0 Loss 0.0000\n",
      "I ll travel across Europe by bicycle this summer .\n",
      "[[2, 51, 755, 862, 763, 61, 662, 18, 363, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 28 Batch 0 Loss 0.0001\n",
      "I met him on my way home .\n",
      "[[2, 304, 48, 23, 17, 101, 73, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 29 Batch 0 Loss 0.0000\n",
      "I think it d be better if you lay down .\n",
      "[[2, 60, 11, 130, 34, 140, 39, 3, 1267, 162, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 30 Batch 0 Loss 0.0378\n",
      "He ordered a chop suey .\n",
      "[[10, 539, 7, 1475, 1476, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 31 Batch 0 Loss 0.0005\n",
      "We dug a hole in the ground .\n",
      "[[26, 1603, 7, 309, 12, 4, 1604, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 32 Batch 0 Loss 0.0002\n",
      "She seems to be possessed by an evil spirit .\n",
      "[[33, 474, 5, 34, 2104, 61, 72, 2105, 2106, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 33 Batch 0 Loss 0.0001\n",
      "Do you think I m too materialistic ?\n",
      "[[22, 3, 60, 2, 36, 93, 1779, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 34 Batch 0 Loss 0.0004\n",
      "I will give you a bike for your birthday .\n",
      "[[2, 50, 108, 3, 7, 644, 19, 25, 202, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 35 Batch 0 Loss 0.0001\n",
      "I want to go to see a movie .\n",
      "[[2, 44, 5, 41, 5, 121, 7, 299, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 36 Batch 0 Loss 0.0001\n",
      "When are we eating ? I m hungry !\n",
      "[[59, 29, 26, 501, 6, 2, 36, 327, 135]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 37 Batch 0 Loss 0.0001\n",
      "Nobody can foresee when the war will end .\n",
      "[[547, 24, 1995, 59, 4, 714, 50, 581, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 38 Batch 0 Loss 0.0009\n",
      "Tom probably can t do that as well as I can .\n",
      "[[27, 467, 24, 9, 22, 15, 66, 169, 66, 2, 24, 1]]\n",
      "tom <end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 39 Batch 0 Loss 0.0001\n",
      "I don t want to get too technical .\n",
      "[[2, 20, 9, 44, 5, 49, 93, 1763, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 40 Batch 0 Loss 0.0001\n",
      "I m thinking about putting my house up for sale .\n",
      "[[2, 36, 466, 68, 2225, 17, 133, 52, 19, 1126, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 41 Batch 0 Loss 0.0001\n",
      "I would rather read a book at home than go out tonight .\n",
      "[[2, 65, 565, 148, 7, 92, 31, 73, 76, 41, 69, 302, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 42 Batch 0 Loss 0.0000\n",
      "It s difficult to teach people what they re not willing to learn .\n",
      "[[11, 14, 228, 5, 505, 117, 28, 55, 75, 32, 959, 5, 221, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 43 Batch 0 Loss 0.0000\n",
      "I have a nap almost every day .\n",
      "[[2, 21, 7, 179, 238, 164, 100, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 44 Batch 0 Loss 0.0001\n",
      " What time is it ? It s . \n",
      "[[28, 46, 8, 11, 6, 11, 14, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 45 Batch 0 Loss 0.0269\n",
      "We re not always at home on Mondays .\n",
      "[[26, 75, 32, 103, 31, 73, 23, 1227, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 46 Batch 0 Loss 0.0001\n",
      "It is quiet here at night .\n",
      "[[11, 8, 989, 58, 31, 124, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 47 Batch 0 Loss 0.0002\n",
      "That actor is both attractive and good at acting .\n",
      "[[15, 2261, 8, 376, 1346, 47, 70, 31, 2262, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 48 Batch 0 Loss 0.0002\n",
      "Look at that building . Is it a temple ?\n",
      "[[145, 31, 15, 562, 1, 8, 11, 7, 1089, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 49 Batch 0 Loss 0.0019\n",
      "That doll is scary .\n",
      "[[15, 548, 8, 996, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 50 Batch 0 Loss 0.0000\n",
      "Give it a shot .\n",
      "[[108, 11, 7, 967, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 51 Batch 0 Loss 0.0000\n",
      "Are you still mad at me ?\n",
      "[[29, 3, 132, 650, 31, 13, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Batch 0 Loss 0.0001\n",
      "Is she a taxi driver ?\n",
      "[[8, 33, 7, 446, 560, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 53 Batch 0 Loss 0.0000\n",
      "You don t understand .\n",
      "[[3, 20, 9, 329, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 54 Batch 0 Loss 0.0000\n",
      "That man knows how to get on the president s good side .\n",
      "[[15, 143, 688, 42, 5, 49, 23, 4, 822, 14, 70, 750, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 55 Batch 0 Loss 0.0001\n",
      "His car has just been repaired .\n",
      "[[37, 119, 54, 98, 107, 1148, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 56 Batch 0 Loss 0.0001\n",
      "I liked walking alone on the deserted beach .\n",
      "[[2, 1293, 945, 390, 23, 4, 2096, 579, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 57 Batch 0 Loss 0.0000\n",
      "Guess how tall I am .\n",
      "[[787, 42, 788, 2, 123, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 58 Batch 0 Loss 0.0000\n",
      "He is always giving presents to his wife .\n",
      "[[10, 8, 103, 1983, 757, 5, 37, 534, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 59 Batch 0 Loss 0.0000\n",
      "I have but one wish .\n",
      "[[2, 21, 97, 81, 226, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 60 Batch 0 Loss 0.0001\n",
      "The teacher has a great influence on his pupils .\n",
      "[[4, 231, 54, 7, 307, 2235, 23, 37, 2236, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 61 Batch 0 Loss 0.0000\n",
      "We both went to the concert .\n",
      "[[26, 376, 168, 5, 4, 553, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 62 Batch 0 Loss 0.0117\n",
      "Did you have a lot of happy experiences in your childhood ?\n",
      "[[43, 3, 21, 7, 158, 16, 509, 2415, 12, 25, 1242, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 63 Batch 0 Loss 0.0000\n",
      "How much is the car you are planning to buy ?\n",
      "[[42, 84, 8, 4, 119, 3, 29, 1305, 5, 78, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 64 Batch 0 Loss 0.0000\n",
      "I am at a loss what to buy him for his birthday .\n",
      "[[2, 123, 31, 7, 2219, 28, 5, 78, 48, 19, 37, 202, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 65 Batch 0 Loss 0.0000\n",
      "I had a bite at around o clock .\n",
      "[[2, 56, 7, 869, 31, 271, 284, 250, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 66 Batch 0 Loss 0.0000\n",
      "I cringe just thinking about it .\n",
      "[[2, 1688, 98, 466, 68, 11, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 67 Batch 0 Loss 0.0000\n",
      "Where will you be this time tomorrow ?\n",
      "[[57, 50, 3, 34, 18, 46, 137, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 68 Batch 0 Loss 0.0001\n",
      "My sister was robbed of her bag on her way home last night .\n",
      "[[17, 396, 30, 802, 16, 53, 437, 23, 53, 101, 73, 79, 124, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 69 Batch 0 Loss 0.0000\n",
      "I am afraid I must be going now .\n",
      "[[2, 123, 385, 2, 151, 34, 94, 67, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 70 Batch 0 Loss 0.0027\n",
      "He left the money at home .\n",
      "[[10, 139, 4, 112, 31, 73, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 71 Batch 0 Loss 0.0002\n",
      "I held my breath in excitement .\n",
      "[[2, 724, 17, 703, 12, 1655, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 72 Batch 0 Loss 0.0000\n",
      "This poem was originally written in French .\n",
      "[[18, 2077, 30, 2078, 941, 12, 184, 1]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 73 Batch 0 Loss 0.0000\n",
      "How much is this tie ?\n",
      "[[42, 84, 8, 18, 1011, 6]]\n",
      "<end>\n",
      "How are you today ?\n",
      "[[42, 29, 3, 118, 6]]\n",
      "<end>\n",
      "Epoch 74 Batch 0 Loss 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-d77d00ec54c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             loss = train_step(source_seq, target_seq_in,\n\u001b[0;32m---> 14\u001b[0;31m                               target_seq_out, en_initial_states)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if encoder_checkpoint is not None and decoder_checkpoint is not None:\n",
    "    encoder.load_weights(encoder_checkpoint)\n",
    "    decoder.load_weights(decoder_checkpoint)\n",
    "\n",
    "if MODE == 'train':\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        en_initial_states = encoder.init_states(BATCH_SIZE)\n",
    "        encoder.save_weights(\n",
    "            'checkpoints_luong/encoder/encoder_{}.h5'.format(e + 1))\n",
    "        decoder.save_weights(\n",
    "            'checkpoints_luong/decoder/decoder_{}.h5'.format(e + 1))\n",
    "        for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "            loss = train_step(source_seq, target_seq_in,\n",
    "                              target_seq_out, en_initial_states)\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    e + 1, batch, loss.numpy()))\n",
    "\n",
    "        try:\n",
    "            predict()\n",
    "\n",
    "            predict(\"How are you today ?\")\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('heatmap'):\n",
    "    os.makedirs('heatmap')\n",
    "\n",
    "test_sents = (\n",
    "    'What a ridiculous concept!',\n",
    "    'Your idea is not entirely crazy.',\n",
    "    \"A man's worth lies in what he is.\",\n",
    "    'What he did is very wrong.',\n",
    "    \"All three of you need to do that.\",\n",
    "    \"Are you giving me another chance?\",\n",
    "    \"Both Tom and Mary work as models.\",\n",
    "    \"Can I have a few minutes, please?\",\n",
    "    \"Could you close the door, please?\",\n",
    "    \"Did you plant pumpkins this year?\",\n",
    "    \"Do you ever study in the library?\",\n",
    "    \"Don't be deceived by appearances.\",\n",
    "    \"Excuse me. Can you speak English?\",\n",
    "    \"Few people know the true meaning.\",\n",
    "    \"Germany produced many scientists.\",\n",
    "    \"Guess whose birthday it is today.\",\n",
    "    \"He acted like he owned the place.\",\n",
    "    \"Honesty will pay in the long run.\",\n",
    "    \"How do we know this isn't a trap?\",\n",
    "    \"I can't believe you're giving up.\",\n",
    ")\n",
    "\n",
    "filenames = []\n",
    "\n",
    "for i, test_sent in enumerate(test_sents):\n",
    "    test_sequence = normalize_string(test_sent)\n",
    "    alignments, source, prediction = predict(test_sequence)\n",
    "    attention = np.squeeze(alignments, (1, 2))\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='jet')\n",
    "    ax.set_xticklabels([''] + source, rotation=90)\n",
    "    ax.set_yticklabels([''] + prediction)\n",
    "\n",
    "    filenames.append('heatmap/test_{}.png'.format(i))\n",
    "    plt.savefig('heatmap/test_{}.png'.format(i))\n",
    "    plt.close()\n",
    "\n",
    "with imageio.get_writer('translation_heatmaps.gif', mode='I', duration=2) as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = (\n",
    "    'What a ridiculous concept!',\n",
    "    'Your idea is not entirely crazy.',\n",
    "    \"A man's worth lies in what he is.\",\n",
    "    'What he did is very wrong.',\n",
    "    \"All three of you need to do that.\",\n",
    "    \"Are you giving me another chance?\",\n",
    "    \"Both Tom and Mary work as models.\",\n",
    "    \"Can I have a few minutes, please?\",\n",
    "    \"Could you close the door, please?\",\n",
    "    \"Did you plant pumpkins this year?\",\n",
    "    \"Do you ever study in the library?\",\n",
    "    \"Don't be deceived by appearances.\",\n",
    "    \"Excuse me. Can you speak English?\",\n",
    "    \"Few people know the true meaning.\",\n",
    "    \"Germany produced many scientists.\",\n",
    "    \"Guess whose birthday it is today.\",\n",
    "    \"He acted like he owned the place.\",\n",
    "    \"Honesty will pay in the long run.\",\n",
    "    \"How do we know this isn't a trap?\",\n",
    "    \"I can't believe you're giving up.\",\n",
    ")\n",
    "\n",
    "for test_sent in test_sents:\n",
    "    test_sequence = normalize_string(test_sent)\n",
    "    predict(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TensorFlow] *",
   "language": "python",
   "name": "conda-env-TensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
