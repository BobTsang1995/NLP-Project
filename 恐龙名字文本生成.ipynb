{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,SimpleRNN,Embedding\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "加载文本获取恐龙名字。创建字符表，计算样本与字符表的长度。(大小写不区分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本长度19909,字符数量27\n"
     ]
    }
   ],
   "source": [
    "data=open('./dinos.txt').read()\n",
    "data=data.lower()\n",
    "char=sorted(set(data))\n",
    "char_num=len(char)\n",
    "print(f'样本长度{len(data)},字符数量{char_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建对照列表。char2id表示字符映射到数字。id2char表示数字映射到字符.\n",
    "'\\n'表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'\\n': 1,\n",
       "  'a': 2,\n",
       "  'b': 3,\n",
       "  'c': 4,\n",
       "  'd': 5,\n",
       "  'e': 6,\n",
       "  'f': 7,\n",
       "  'g': 8,\n",
       "  'h': 9,\n",
       "  'i': 10,\n",
       "  'j': 11,\n",
       "  'k': 12,\n",
       "  'l': 13,\n",
       "  'm': 14,\n",
       "  'n': 15,\n",
       "  'o': 16,\n",
       "  'p': 17,\n",
       "  'q': 18,\n",
       "  'r': 19,\n",
       "  's': 20,\n",
       "  't': 21,\n",
       "  'u': 22,\n",
       "  'v': 23,\n",
       "  'w': 24,\n",
       "  'x': 25,\n",
       "  'y': 26,\n",
       "  'z': 27},\n",
       " {1: '\\n',\n",
       "  2: 'a',\n",
       "  3: 'b',\n",
       "  4: 'c',\n",
       "  5: 'd',\n",
       "  6: 'e',\n",
       "  7: 'f',\n",
       "  8: 'g',\n",
       "  9: 'h',\n",
       "  10: 'i',\n",
       "  11: 'j',\n",
       "  12: 'k',\n",
       "  13: 'l',\n",
       "  14: 'm',\n",
       "  15: 'n',\n",
       "  16: 'o',\n",
       "  17: 'p',\n",
       "  18: 'q',\n",
       "  19: 'r',\n",
       "  20: 's',\n",
       "  21: 't',\n",
       "  22: 'u',\n",
       "  23: 'v',\n",
       "  24: 'w',\n",
       "  25: 'x',\n",
       "  26: 'y',\n",
       "  27: 'z'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2id={i:u+1 for u,i in enumerate(char)}\n",
    "id2char={u+1:i for u,i in enumerate(char)}\n",
    "char2id,id2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aachenosaurus'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./dinos.txt') as f:\n",
    "    examples = f.readlines()\n",
    "examples = [x.lower().strip() for x in examples]\n",
    "maxlen=max([len(i) for i in examples ])\n",
    "examples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练集的字符变为数字编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2, 4, 9, 6, 15, 16, 20, 2, 22, 19, 22, 20],\n",
       " [2, 4, 9, 6, 15, 16, 20, 2, 22, 19, 22, 20, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y=[],[]\n",
    "for index in range(len(examples)):\n",
    "    x =[char2id[ch] for ch in examples[index]]\n",
    "    y =x[1:]+[char2id[\"\\n\"]]\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "X[0],Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将输出padding为同一长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bob/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/bob/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X=np.array(X)\n",
    "Y=np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X=tf.keras.preprocessing.sequence.pad_sequences(X,maxlen=maxlen,padding='post',value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_Y=tf.keras.preprocessing.sequence.pad_sequences(Y,maxlen=maxlen,padding='post',value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536, 26) (1536, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 2,  2,  4,  9,  6, 15, 16, 20,  2, 22, 19, 22, 20,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32),\n",
       " array([ 2,  4,  9,  6, 15, 16, 20,  2, 22, 19, 22, 20,  1,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(padded_X.shape,padded_Y.shape)\n",
    "padded_X[0], padded_Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练集随机打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "np.random.shuffle(X)\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([23, 16, 22, 10, 23, 19, 10, 2], [16, 22, 10, 23, 19, 10, 2, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3],Y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(padded_X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((padded_X, padded_Y))\n",
    "train_db=train_db.batch(32,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: (32, 26) (32, 26)\n",
      "tf.Tensor(\n",
      "[ 2  2  4  9  6 15 16 20  2 22 19 22 20  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0], shape=(26,), dtype=int32) tf.Tensor(\n",
      "[ 2  4  9  6 15 16 20  2 22 19 22 20  1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0], shape=(26,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_db)\n",
    "# next() 返回迭代器的下一个项目\n",
    "sample = next(train_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)\n",
    "print(sample[0][0],sample[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型\n",
    "创建模型，注意embdding层是vocab_size+1，应为加入了padding 0. 最后的softmax 也是vocab_size+1 注意是return_sequences=True,应为每一个时刻我们都要产生输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_model(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,rnn_units):\n",
    "        super(My_model,self).__init__()\n",
    "        self.embedding=Embedding(vocab_size+1,5,name='emb')\n",
    "        self.rnn=SimpleRNN(rnn_units,return_sequences=True,name='rnn')\n",
    "#         self.d1=Dense(64,activation='relu',name='d1')\n",
    "        self.d2=Dense(vocab_size+1,activation='softmax',name='d2')\n",
    "    \n",
    "    def call(self,x):\n",
    "        x=self.embedding(x)\n",
    "        x=self.rnn(x)\n",
    "#         x=self.d1(x)\n",
    "        x=self.d2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=My_model(char_num,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取样\n",
    "我们在一个时刻会得到一个预测，然后我们需要将这个预测结果作为下一个时间点的输入，然后进行下一次预测。 我们输出的yt是softmax之后的结果，代表我们预测下一个单词的概率，然后我们需要依照概率进行抽样（注意不像往常依照取argmax，因为这样我们很有可能产生死循环）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample(model):\n",
    "    seed=0\n",
    "    name=[]\n",
    "    for i in range(5):\n",
    "        a=[random.randint(1,27)]\n",
    "        b=tf.expand_dims(a,0)\n",
    "        ans=[id2char[a[0]].upper()]\n",
    "        for i in range(20):\n",
    "            pred=model(b)\n",
    "            pred=tf.squeeze(pred)\n",
    "            pred=np.array(pred)\n",
    "            \n",
    "            # for grading purposes\n",
    "            np.random.seed(i+seed) \n",
    "        \n",
    "            idx = np.random.choice(list(range(28)), p=pred.ravel())\n",
    "            if idx==0 or idx==1:\n",
    "                break\n",
    "            next_word=id2char[idx]\n",
    "            ans.append(next_word)\n",
    "            a=[char2id[next_word]]\n",
    "            b=tf.expand_dims(a,0)\n",
    "            seed+=1\n",
    "        \n",
    "        ans=''.join(ans)\n",
    "        name.append(ans)\n",
    "    for n in name:\n",
    "        if n is not None:\n",
    "            print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义优化器和损失\n",
    "我们要将padding 0 位置上产生的损失mask掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer=tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "def loss_function(y_true,y_pred):\n",
    "    # 我们将0mask掉，不计算0的损失\n",
    "    mask=tf.math.logical_not(tf.math.equal(y_true,0))\n",
    "    loss=loss_object(y_true,y_pred)\n",
    "    mask=tf.cast(mask,dtype=loss.dtype)\n",
    "    loss*=mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行训练\n",
    "训练的时候，我们不像预测进行采样，因为训练的时候，我们预测的结果很有可能是错的，然后我们传入错误的结果进行预测，那么产生的下一个结果就更加糟糕了。\n",
    "\n",
    "所以我们使用强制教学（teaching force）的方式。将下一个正确的答案输入到模型，然后进行下一次的预测。 此外，我们需要对模型进行梯度裁剪，避免梯度爆炸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp,targ):\n",
    "    loss=0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        \n",
    "        # 强制教学-将目标词作为下一个输入\n",
    "        model_input=inp[:,0]\n",
    "        model_input=tf.expand_dims(model_input,1)\n",
    "        for t in range(1,targ.shape[1]):\n",
    "            # 将编码器输出传到解码器\n",
    "            predictions=model(model_input)\n",
    "            \n",
    "            loss+=loss_function(targ[:,t],predictions)\n",
    "            \n",
    "            # 使用强制教学\n",
    "            model_input=tf.expand_dims(targ[:,t],1)\n",
    "        \n",
    "        batch_loss=(loss/int(targ.shape[1]))\n",
    "        \n",
    "        variables=model.variables\n",
    "\n",
    "        # 对每一个变量计算梯度\n",
    "        gradients=tape.gradient(loss,variables)\n",
    "        \n",
    "#         print(type(gradients))\n",
    "#         print(gradients[0])\n",
    "#         print(gradients[1])\n",
    "        \n",
    "        gradients, _ = tf.clip_by_global_norm(gradients,3)\n",
    "#         gradients = [tf.clip_by_value(gards, -3, 3) for gards in gradients if gards is not None]\n",
    "\n",
    "        # Apply gradients to variables\n",
    "        optimizer.apply_gradients(zip(gradients,variables))\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 66.5260\n",
      "Time taken for 1 epoch 20.905553102493286 sec\n",
      "\n",
      "Nnkzwwtcmeqodygsqv\n",
      "Yga\n",
      "Ga\n",
      "Eodygsqv\n",
      "Vjvfekyneazagqvaaxune\n",
      "\n",
      "Epoch 2 Loss 63.5822\n",
      "Time taken for 1 epoch 0.4968221187591553 sec\n",
      "\n",
      "Lnkywvsamdqodygsqv\n",
      "Afa\n",
      "Ua\n",
      "Todygsqv\n",
      "Vjvfelxneazagrua\n",
      "\n",
      "Epoch 3 Loss 58.9736\n",
      "Time taken for 1 epoch 0.5789370536804199 sec\n",
      "\n",
      "Golyuusancqodxfsqu\n",
      "Eg\n",
      "Yrodxfsqu\n",
      "Kfsqu\n",
      "Hqu\n",
      "\n",
      "Epoch 4 Loss 53.8035\n",
      "Time taken for 1 epoch 0.5734400749206543 sec\n",
      "\n",
      "Conxustanaqoewfsos\n",
      "Ge\n",
      "Hqoewfsos\n",
      "Vfsos\n",
      "Hqu\n",
      "\n",
      "Epoch 5 Loss 49.4197\n",
      "Time taken for 1 epoch 0.5708789825439453 sec\n",
      "\n",
      "Xnjxuss\n",
      "Xuss\n",
      "K\n",
      "G\n",
      "P\n",
      "\n",
      "Epoch 6 Loss 46.5001\n",
      "Time taken for 1 epoch 0.5532689094543457 sec\n",
      "\n",
      "Eriwusranapohuosmu\n",
      "Cga\n",
      "Da\n",
      "Lohuosmu\n",
      "Qitfeouraayaeruaausfd\n",
      "\n",
      "Epoch 7 Loss 44.8787\n",
      "Time taken for 1 epoch 0.5661008358001709 sec\n",
      "\n",
      "Xlmwusoeperoiurusu\n",
      "Lga\n",
      "Ja\n",
      "Doiurusu\n",
      "Cluqbnvolayaerucausac\n",
      "\n",
      "Epoch 8 Loss 43.9297\n",
      "Time taken for 1 epoch 0.5550987720489502 sec\n",
      "\n",
      "Ropvusnanbpojvhscuasa\n",
      "Mojvhscuasailus\n",
      "Q\n",
      "E\n",
      "D\n",
      "\n",
      "Epoch 9 Loss 43.3135\n",
      "Time taken for 1 epoch 0.5543279647827148 sec\n",
      "\n",
      "Kopvusnanbpokuruss\n",
      "Mea\n",
      "S\n",
      "Ma\n",
      "Hokuruss\n",
      "\n",
      "Epoch 10 Loss 42.8603\n",
      "Time taken for 1 epoch 0.5560598373413086 sec\n",
      "\n",
      "Copvusmanbpokuruss\n",
      "Zha\n",
      "Ra\n",
      "Aokuruss\n",
      "Ynureotolayafrulausac\n",
      "\n",
      "Epoch 11 Loss 42.4839\n",
      "Time taken for 1 epoch 0.5618760585784912 sec\n",
      "\n",
      "Ropvusmanbpoluruss\n",
      "Xaa\n",
      "Qa\n",
      "Zoluruss\n",
      "Saureotomayagptaausac\n",
      "\n",
      "Epoch 12 Loss 42.1444\n",
      "Time taken for 1 epoch 0.5550661087036133 sec\n",
      "\n",
      "Nniustranboravhrus\n",
      "Qea\n",
      "Qa\n",
      "Coluruss\n",
      "Qitidotomayagos\n",
      "\n",
      "Epoch 13 Loss 41.8298\n",
      "Time taken for 1 epoch 0.5558168888092041 sec\n",
      "\n",
      "Samustranasaduruss\n",
      "Ona\n",
      "Qa\n",
      "Zoluruss\n",
      "Xaureotomayagos\n",
      "\n",
      "Epoch 14 Loss 41.5450\n",
      "Time taken for 1 epoch 0.5580141544342041 sec\n",
      "\n",
      "Erovusmaolos\n",
      "\n",
      "\n",
      "Cesrga\n",
      "Aerteuruss\n",
      "Gurureotomayahos\n",
      "\n",
      "Epoch 15 Loss 41.3013\n",
      "Time taken for 1 epoch 0.5606088638305664 sec\n",
      "\n",
      "Xgiusslbolos\n",
      "Zdrura\n",
      "Perteurusr\n",
      "Qurureotonayairun\n",
      "Agayairun\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS=15\n",
    "\n",
    "steps_per_epoch=len(X)//32\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    total_loss=0\n",
    "    \n",
    "    for (batch,(inp,targ)) in enumerate(train_db.take(steps_per_epoch)):\n",
    "        batch_loss=train_step(inp,targ)\n",
    "        total_loss+=batch_loss\n",
    "     \n",
    "        # 每 2 个周期（epoch），保存（检查点）一次模型\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "#         checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss ))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    # 在每一次训练周期进行输出，可以查看一开始生成的名字乱七八糟，后来的名字逐渐有规律了\n",
    "    sample(model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TensorFlow] *",
   "language": "python",
   "name": "conda-env-TensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
