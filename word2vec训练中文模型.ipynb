{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# code by BobTsang \n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "from gensim.corpora import WikiCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为原始文件是xml格式，并且是压缩文件，所以做了一步数据解压并进行格式转换的工作。\\\n",
    "具体使用了gensim库中的维基百科处理类WikiCorpus，该类中的get_texts方法原文件中的文章转化为一个数组，其中每一个元素对应着原文件中的一篇文章。然后通过for循环便可以将其中的每一篇文章读出，然后进行保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主程序开始...\n",
      "开始读入wiki数据...\n",
      "wiki数据读入完成！\n",
      "处理程序开始...\n",
      "目前已处理10000条数据\n",
      "目前已处理20000条数据\n",
      "目前已处理30000条数据\n",
      "目前已处理40000条数据\n",
      "目前已处理50000条数据\n",
      "目前已处理60000条数据\n",
      "目前已处理70000条数据\n",
      "目前已处理80000条数据\n",
      "目前已处理90000条数据\n",
      "目前已处理100000条数据\n",
      "目前已处理110000条数据\n",
      "目前已处理120000条数据\n",
      "目前已处理130000条数据\n",
      "目前已处理140000条数据\n",
      "目前已处理150000条数据\n",
      "目前已处理160000条数据\n",
      "目前已处理170000条数据\n",
      "目前已处理180000条数据\n",
      "目前已处理190000条数据\n",
      "目前已处理200000条数据\n",
      "目前已处理210000条数据\n",
      "目前已处理220000条数据\n",
      "目前已处理230000条数据\n",
      "目前已处理240000条数据\n",
      "目前已处理250000条数据\n",
      "目前已处理260000条数据\n",
      "目前已处理270000条数据\n",
      "目前已处理280000条数据\n",
      "目前已处理290000条数据\n",
      "目前已处理300000条数据\n",
      "目前已处理310000条数据\n",
      "目前已处理320000条数据\n",
      "目前已处理330000条数据\n",
      "处理程序结束！\n",
      "主程序结束！\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print('主程序开始...')\n",
    "    # 定义输入输出\n",
    "    input_file_name = 'zhwiki-latest-pages-articles.xml.bz2'\n",
    "    output_file_name = 'wiki.zh.text'\n",
    "    \n",
    "    print('开始读入wiki数据...')\n",
    "    input_file = WikiCorpus(input_file_name, lemmatize=False, dictionary={})\n",
    "    print('wiki数据读入完成！')\n",
    "    output_file = open(output_file_name, 'w', encoding=\"utf-8\")\n",
    "    \n",
    "    print('处理程序开始...')\n",
    "    count = 0\n",
    "    for text in input_file.get_texts():\n",
    "        output_file.write(' '.join(text) + '\\n')\n",
    "        count = count + 1\n",
    "        if count % 10000 == 0:\n",
    "            print('目前已处理%d条数据' % count)\n",
    "    print('处理程序结束！')\n",
    "\n",
    "    output_file.close()\n",
    "    print('主程序结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 繁体转为简体\n",
    "为了方便后期处理，接下来对上面的结果进行简体化处理，将所有的繁体全部转化为简体。在这里，使用了另外一个库zhconv。对上面结果的每一行调用convert函数即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主程序执行开始...\n",
      "开始读入繁体文件...\n",
      "读入繁体文件结束！\n",
      "转换程序执行开始...\n",
      "目前已转换10000条数据\n",
      "目前已转换20000条数据\n",
      "目前已转换30000条数据\n",
      "目前已转换40000条数据\n",
      "目前已转换50000条数据\n",
      "目前已转换60000条数据\n",
      "目前已转换70000条数据\n",
      "目前已转换80000条数据\n",
      "目前已转换90000条数据\n",
      "目前已转换100000条数据\n",
      "目前已转换110000条数据\n",
      "目前已转换120000条数据\n",
      "目前已转换130000条数据\n",
      "目前已转换140000条数据\n",
      "目前已转换150000条数据\n",
      "目前已转换160000条数据\n",
      "目前已转换170000条数据\n",
      "目前已转换180000条数据\n",
      "目前已转换190000条数据\n",
      "目前已转换200000条数据\n",
      "目前已转换210000条数据\n",
      "目前已转换220000条数据\n",
      "目前已转换230000条数据\n",
      "目前已转换240000条数据\n",
      "目前已转换250000条数据\n",
      "目前已转换260000条数据\n",
      "目前已转换270000条数据\n",
      "目前已转换280000条数据\n",
      "目前已转换290000条数据\n",
      "目前已转换300000条数据\n",
      "目前已转换310000条数据\n",
      "目前已转换320000条数据\n",
      "目前已转换330000条数据\n",
      "转换程序执行结束！\n",
      "主程序执行结束！\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import zhconv\n",
    "\n",
    "print('主程序执行开始...')\n",
    "\n",
    "input_file_name = 'wiki.zh.text'\n",
    "output_file_name = 'wiki.cn.simple.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入繁体文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入繁体文件结束！')\n",
    "\n",
    "print('转换程序执行开始...')\n",
    "count = 1\n",
    "for line in lines:\n",
    "    output_file.write(zhconv.convert(line, 'zh-hans'))\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print('目前已转换%d条数据' % count)\n",
    "print('转换程序执行结束！')\n",
    "\n",
    "print('主程序执行结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.训练数据\n",
    "Python的话可用jieba完成分词，生成分词文件wiki.zh.text.seg 接着用word2vec工具训练：\\\n",
    "对于中文来说，分词是必须要经过的一步处理，下面就需要进行分词操作。在这里使用了大名鼎鼎的jieba库。调用其中的cut方法即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主程序执行开始...\n",
      "开始读入数据文件...\n",
      "读入数据文件结束！\n",
      "分词程序执行开始...\n",
      "目前已分词10000条数据\n",
      "目前已分词20000条数据\n",
      "目前已分词30000条数据\n",
      "目前已分词40000条数据\n",
      "目前已分词50000条数据\n",
      "目前已分词60000条数据\n",
      "目前已分词70000条数据\n",
      "目前已分词80000条数据\n",
      "目前已分词90000条数据\n",
      "目前已分词100000条数据\n",
      "目前已分词110000条数据\n",
      "目前已分词120000条数据\n",
      "目前已分词130000条数据\n",
      "目前已分词140000条数据\n",
      "目前已分词150000条数据\n",
      "目前已分词160000条数据\n",
      "目前已分词170000条数据\n",
      "目前已分词180000条数据\n",
      "目前已分词190000条数据\n",
      "目前已分词200000条数据\n",
      "目前已分词210000条数据\n",
      "目前已分词220000条数据\n",
      "目前已分词230000条数据\n",
      "目前已分词240000条数据\n",
      "目前已分词250000条数据\n",
      "目前已分词260000条数据\n",
      "目前已分词270000条数据\n",
      "目前已分词280000条数据\n",
      "目前已分词290000条数据\n",
      "目前已分词300000条数据\n",
      "目前已分词310000条数据\n",
      "目前已分词320000条数据\n",
      "目前已分词330000条数据\n",
      "分词程序执行结束！\n",
      "主程序执行结束！\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import jieba\n",
    "\n",
    "print('主程序执行开始...')\n",
    "\n",
    "input_file_name = 'wiki.cn.simple.txt'\n",
    "output_file_name = 'wiki.cn.simple.separate.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入数据文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入数据文件结束！')\n",
    "\n",
    "print('分词程序执行开始...')\n",
    "count = 1\n",
    "for line in lines:\n",
    "    # jieba分词的结果是一个list，需要拼接，但是jieba把空格回车都当成一个字符处理\n",
    "    output_file.write(' '.join(jieba.cut(line.split('\\n')[0].replace(' ', ''))) + '\\n')\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print('目前已分词%d条数据' % count)\n",
    "print('分词程序执行结束！')\n",
    "\n",
    "print('主程序执行结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除非中文词\n",
    "可以看到，经过上面的处理之后，现在的结果已经差不多了，但是还存在着一些非中文词，所以下一步便将这些词去除。具体做法是通过正则表达式判断每一个词是不是符合汉字开头、汉字结尾、中间全是汉字，即“^[\\u4e00-\\u9fa5]+$”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主程序执行开始...\n",
      "开始读入数据文件...\n",
      "读入数据文件结束！\n",
      "分词程序执行开始...\n",
      "目前已分词10000条数据\n",
      "目前已分词20000条数据\n",
      "目前已分词30000条数据\n",
      "目前已分词40000条数据\n",
      "目前已分词50000条数据\n",
      "目前已分词60000条数据\n",
      "目前已分词70000条数据\n",
      "目前已分词80000条数据\n",
      "目前已分词90000条数据\n",
      "目前已分词100000条数据\n",
      "目前已分词110000条数据\n",
      "目前已分词120000条数据\n",
      "目前已分词130000条数据\n",
      "目前已分词140000条数据\n",
      "目前已分词150000条数据\n",
      "目前已分词160000条数据\n",
      "目前已分词170000条数据\n",
      "目前已分词180000条数据\n",
      "目前已分词190000条数据\n",
      "目前已分词200000条数据\n",
      "目前已分词210000条数据\n",
      "目前已分词220000条数据\n",
      "目前已分词230000条数据\n",
      "目前已分词240000条数据\n",
      "目前已分词250000条数据\n",
      "目前已分词260000条数据\n",
      "目前已分词270000条数据\n",
      "目前已分词280000条数据\n",
      "目前已分词290000条数据\n",
      "目前已分词300000条数据\n",
      "目前已分词310000条数据\n",
      "目前已分词320000条数据\n",
      "目前已分词330000条数据\n",
      "分词程序执行结束！\n",
      "主程序执行结束！\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import re\n",
    "\n",
    "print('主程序执行开始...')\n",
    "\n",
    "input_file_name = 'wiki.cn.simple.separate.txt'\n",
    "output_file_name = 'wiki.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入数据文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入数据文件结束！')\n",
    "\n",
    "print('分词程序执行开始...')\n",
    "count = 1\n",
    "cn_reg = '^[\\u4e00-\\u9fa5]+$'\n",
    "\n",
    "for line in lines:\n",
    "    line_list = line.split('\\n')[0].split(' ')\n",
    "    line_list_new = []\n",
    "    for word in line_list:\n",
    "        if re.search(cn_reg, word):\n",
    "            line_list_new.append(word)\n",
    "    # print(line_list_new)\n",
    "    output_file.write(' '.join(line_list_new) + '\\n')\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print('目前已分词%d条数据' % count)\n",
    "print('分词程序执行结束！')\n",
    "\n",
    "print('主程序执行结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量训练\n",
    "上面的工作主要是对wiki语料库进行数据预处理，接下来才真正的词向量训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主程序开始执行...\n",
      "转换过程开始...\n",
      "转换过程结束！\n",
      "开始保存模型...\n",
      "模型保存结束！\n",
      "主程序执行结束！\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('主程序开始执行...')\n",
    "\n",
    "    input_file_name = 'wiki.txt'\n",
    "    model_file_name = 'wiki.model'\n",
    "\n",
    "    print('转换过程开始...')\n",
    "    model = Word2Vec(LineSentence(input_file_name),\n",
    "                     size=400,  # 词向量长度为400\n",
    "                     window=5,\n",
    "                     min_count=5,\n",
    "                     workers=multiprocessing.cpu_count())\n",
    "    print('转换过程结束！')\n",
    "\n",
    "    print('开始保存模型...')\n",
    "    model.save(model_file_name)\n",
    "    print('模型保存结束！')\n",
    "\n",
    "    print('主程序执行结束！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bob/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女人 0.7586730718612671\n",
      "家伙 0.6032949686050415\n",
      "傻瓜 0.5945301651954651\n",
      "小伙子 0.5009162425994873\n",
      "女孩儿 0.48738691210746765\n",
      "爸爸 0.48539841175079346\n",
      "陌生人 0.48290586471557617\n",
      "老公 0.47928890585899353\n",
      "坏女孩 0.4747428894042969\n",
      "女孩子 0.4738668203353882\n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "result = model.most_similar(u\"男人\")\n",
    "for e in result:\n",
    "    print(e[0], e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美式足球 0.6555909514427185\n",
      "男子篮球 0.608197808265686\n",
      "橄榄球 0.6055187582969666\n",
      "棒球 0.5942047834396362\n",
      "排球 0.5929747223854065\n",
      "冰球 0.5831517577171326\n",
      "篮球员 0.5530334115028381\n",
      "足球 0.5451972484588623\n",
      "篮球队 0.5413496494293213\n",
      "篮球运动 0.5324910879135132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bob/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "result = model.most_similar(u\"篮球\")\n",
    "for e in result:\n",
    "    print(e[0], e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试结果\n",
    "result = model.most_similar(u\"曾国藩\")\n",
    "for e in result:\n",
    "    print(e[0], e[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试结果\n",
    "result = model.most_similar(u\"薯片\")\n",
    "for e in result:\n",
    "    print(e[0], e[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试结果\n",
    "result = model.most_similar(u\"中国\")\n",
    "for e in result:\n",
    "    print(e[0], e[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试结果\n",
    "result = model.most_similar(u\"北京邮电大学\")\n",
    "for e in result:\n",
    "    print(e[0], e[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "左宗棠 0.8331661820411682\n",
      "胡林翼 0.8123552203178406\n",
      "彭玉麟 0.7856593132019043\n",
      "曾国荃 0.7785587310791016\n",
      "张之洞 0.7551987171173096\n",
      "郭嵩焘 0.74876868724823\n",
      "骆秉章 0.747631311416626\n",
      "端方 0.7401894927024841\n",
      "李鸿章 0.728326678276062\n",
      "林则徐 0.72769695520401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bob/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "奶茶 0.8098530769348145\n",
      "薯条 0.7850062847137451\n",
      "酸奶 0.7830050587654114\n",
      "果汁 0.7578129768371582\n",
      "麦片 0.7564541101455688\n",
      "洋芋片 0.7530785799026489\n",
      "橙汁 0.7526496648788452\n",
      "鲜奶 0.7514513731002808\n",
      "无糖 0.7508646249771118\n",
      "午餐肉 0.7481974363327026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bob/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "result = model.most_similar(u\"薯片\")\n",
    "for e in result:\n",
    "    print(e[0], e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bob/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我国 0.5576021671295166\n",
      "中国政府 0.48120301961898804\n",
      "礁层 0.4699247479438782\n",
      "北京 0.4657849371433258\n",
      "海外华人 0.4390381872653961\n",
      "台湾 0.4375913143157959\n",
      "上海 0.4287085235118866\n",
      "日本 0.42295077443122864\n",
      "张玉嬿萧 0.4228498935699463\n",
      "中华民族 0.42100322246551514\n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "result = model.most_similar(u\"中国\")\n",
    "for e in result:\n",
    "    print(e[0], e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京科技大学 0.8199517130851746\n",
      "西安交通大学 0.8131524324417114\n",
      "北京航空航天大学 0.8054287433624268\n",
      "武汉理工大学 0.8020358681678772\n",
      "北京工业大学 0.7956100702285767\n",
      "北京理工大学 0.7944455146789551\n",
      "大连理工大学 0.7923593521118164\n",
      "南京师范大学 0.791751503944397\n",
      "首都师范大学 0.7916684746742249\n",
      "华中师范大学 0.7892749309539795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bob/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "result = model.most_similar(u\"北京邮电大学\")\n",
    "for e in result:\n",
    "    print(e[0], e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}